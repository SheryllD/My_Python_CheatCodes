{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for Data Analysis with Python\n",
    "\n",
    "# 1. Steps and codes for data analysis\n",
    "1. Import Python Libraries \n",
    "2. Reading Dataset\n",
    "3. Data Reduction\n",
    "4. Feature Engineering\n",
    "5. Creating Features \n",
    "6. Data Cleaning and Wrangling \n",
    "7. EDA Exploratory Data Analysis \n",
    "\n",
    "\n",
    "# 1.1 Import Python Libraries \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "To ignore warnings you can use: \n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 2 Reading Dataset \n",
    "\n",
    "## 2.2.1 Load Dataset \n",
    "The Pandas library offers a wide range of possibilities for loading data into the pandas DataFrame from files like JSON, .csv, .xlsx, .sql, .pickle, .html, .txt, images etc.\n",
    "\n",
    "#### To Load CSV File \n",
    "Most data are available in a tabular format of CSV file. To load data you can do the following: \n",
    "\n",
    "(df for dataframe)\n",
    "df = pd.read_csv(\"add here the csv file\")\n",
    "\n",
    "#### To Load JSON file\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"file.json\", \"r\") as file:\n",
    "    df = json.load(file)\n",
    "\n",
    "print(df)  # Print loaded JSON data\n",
    "\n",
    "#### To Load XLSX file\n",
    "\n",
    "import pandas as pd  # if you haven't already\n",
    "\n",
    "df = pd.read_excel(\"file.xlsx\")  # Load Excel file\n",
    "\n",
    "#### To Load SQL file\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"database.db\")  # Connect to SQLite database\n",
    "cursor = conn.cursor()\n",
    "\n",
    "with open(\"file.sql\", \"r\") as file:\n",
    "    sql_script = file.read()  # Read SQL script\n",
    "\n",
    "cursor.executescript(sql_script)  # Execute SQL script\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "#### To Load PICKLE file\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"file.pkl\", \"rb\") as file:\n",
    "    df = pickle.load(file)\n",
    "\n",
    "print(df)  # Display the loaded object\n",
    "\n",
    "#### To Load HTML \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"file.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "print(soup.prettify())  # Print formatted HTML content\n",
    "\n",
    "#### To Load TXT file\n",
    "\n",
    "with open(\"file.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text)  # Print text content\n",
    "\n",
    "#### To Load IMAGE file\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"file.jpg\")  # Change the file extension if needed (png, bmp, etc.)\n",
    "image.show()  # Open the image\n",
    "\n",
    "## 2.2.2 Analyse the Data\n",
    "\n",
    "df.head() # displaying the top 5 \n",
    "\n",
    "df.tail() # displaying the last 5 \n",
    "\n",
    "df.info #\n",
    "\n",
    "df.nunique() # displaying several unique values in each column and the data description, we can identify the continuous and categorical columns in the data. Check also for duplicates. \n",
    "\n",
    "### 2.2.3 Check for Missing Values \n",
    "\n",
    "df.isnull()\n",
    "\n",
    "df.isna()\n",
    "\n",
    "data.isnull()sum() # get the number of missing records in each column \n",
    "\n",
    "To calculate the percentage of missing values in each column \n",
    "\n",
    "(df.isnull().sum() /(len(data))) * 100\n",
    "\n",
    "# 3. Data Reduction \n",
    "\n",
    "## 3.1 Start Removing Data Columns \n",
    "\n",
    "Review which columns you can drop. \n",
    "\n",
    "df = data.drop([\"INSER_COLUMN_NAME\"], axis = 1)\n",
    "df.info #review the data again. \n",
    "\n",
    "# 4. Feature Engineering \n",
    "\n",
    "\n",
    "# 5. Creating Features \n",
    "\n",
    "# 6. Data Cleaning and Wrangling \n",
    "\n",
    "# 7. EDA Exploratory Data Analysis \n",
    "\n",
    "# 8. Statistics Summary \n",
    "\n",
    "\n",
    "## Sources for further information: \n",
    "1. https://www.analyticsvidhya.com/blog/2022/07/step-by-step-exploratory-data-analysis-eda-using-python/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
